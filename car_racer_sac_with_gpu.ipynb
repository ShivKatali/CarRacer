{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A4_NA9Y1cIC0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (2.1.1)\n",
            "Requirement already satisfied: filelock in /home/fabian/.local/lib/python3.9/site-packages (from torch) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: sympy in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/fabian/.local/lib/python3.9/site-packages (from torch) (2.6.2)\n",
            "Requirement already satisfied: jinja2 in /home/fabian/.local/lib/python3.9/site-packages (from torch) (3.0.1)\n",
            "Requirement already satisfied: fsspec in /home/fabian/.local/lib/python3.9/site-packages (from torch) (2021.8.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/fabian/.local/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: swig in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (4.1.1.post1)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gymnasium in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium) (1.26.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/fabian/.local/lib/python3.9/site-packages (from gymnasium) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium) (4.8.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium) (6.8.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gymnasium[box2d] in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (1.26.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/fabian/.local/lib/python3.9/site-packages (from gymnasium[box2d]) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (4.8.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (6.8.0)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (4.1.1.post1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.17.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: matplotlib in /home/fabian/.local/lib/python3.9/site-packages (3.3.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/fabian/.local/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/fabian/.local/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (1.26.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (10.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/fabian/.local/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: imageio in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (2.33.0)\n",
            "Requirement already satisfied: numpy in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from imageio) (1.26.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from imageio) (10.1.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: envpool in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (0.8.4)\n",
            "Requirement already satisfied: dm-env>=1.4 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from envpool) (1.6)\n",
            "Requirement already satisfied: gym>=0.18 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from envpool) (0.26.2)\n",
            "Requirement already satisfied: gymnasium>=0.26 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from envpool) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.19 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from envpool) (1.26.2)\n",
            "Requirement already satisfied: types-protobuf>=3.17.3 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from envpool) (4.24.0.20240129)\n",
            "Requirement already satisfied: typing-extensions in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from envpool) (4.8.0)\n",
            "Requirement already satisfied: packaging in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from envpool) (23.2)\n",
            "Requirement already satisfied: optree>=0.6.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from envpool) (0.10.0)\n",
            "Requirement already satisfied: absl-py in /home/fabian/.local/lib/python3.9/site-packages (from dm-env>=1.4->envpool) (1.0.0)\n",
            "Requirement already satisfied: dm-tree in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from dm-env>=1.4->envpool) (0.1.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/fabian/.local/lib/python3.9/site-packages (from gym>=0.18->envpool) (1.6.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gym>=0.18->envpool) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gym>=0.18->envpool) (6.8.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium>=0.26->envpool) (0.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym>=0.18->envpool) (3.17.0)\n",
            "Requirement already satisfied: six in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from absl-py->dm-env>=1.4->envpool) (1.16.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: optuna in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (3.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from optuna) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from optuna) (2.0.25)\n",
            "Requirement already satisfied: tqdm in /home/fabian/.local/lib/python3.9/site-packages (from optuna) (4.62.2)\n",
            "Requirement already satisfied: PyYAML in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.8.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/fabian/.local/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install torch\n",
        "!pip install swig\n",
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "!pip install matplotlib\n",
        "!pip install imageio\n",
        "!pip install envpool\n",
        "!pip install optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ppu3DNtccIC1"
      },
      "outputs": [],
      "source": [
        "!# Imports\n",
        "from typing import Iterable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from PIL import Image\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Callable\n",
        "from collections import namedtuple\n",
        "import itertools\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzNBit4YTuC1",
        "outputId": "d1b4017e-f2ec-4176-cbea-1adebaeab271"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USE CPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU Available!\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"USE CPU\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VlP3XLDvcIC3"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim: torch.Size, action_dim: torch.Size):\n",
        "        \"\"\"\n",
        "        Initialize the Critic network.\n",
        "\n",
        "        :param obs_dim: dimention of the observations\n",
        "        :param num_actions: dimention of the actions\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(obs_dim[-1], 16, 5, stride=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1)\n",
        "        self.fc1 = nn.Linear(32*(obs_dim[0]-6)*(obs_dim[1]-6) + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        x = obs.permute(0, 3, 1, 2)\n",
        "        \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.cat([x, action], dim=-1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        out = self.fc3(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2j5bfzwtcIC4"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim: torch.Size, action_dim: torch.Size, action_low: np.array, action_high: np.array):\n",
        "        \"\"\"\n",
        "        Initialize the Actor network.\n",
        "\n",
        "        :param obs_dim: dimention of the observations\n",
        "        :param num_actions: dimention of the actions\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        # We are registering scale and bias as buffers so they can be saved and loaded as part of the model.\n",
        "        # Buffers won't be passed to the optimizer for training!\n",
        "        self.register_buffer(\n",
        "            \"action_scale\", torch.tensor((action_high - action_low) / 2.0, dtype=torch.float32)\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"action_bias\", torch.tensor((action_high + action_low) / 2.0, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "        # TODO: your code\n",
        "        self.conv1 = nn.Conv2d(obs_dim[-1], 16, 5, stride=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1)\n",
        "        self.fc1 = nn.Linear(32*(obs_dim[0]-6)*(obs_dim[1]-6), 256)\n",
        "        self.fc_mu = nn.Linear(256, action_dim)\n",
        "        self.fc_std = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the actor network.\n",
        "\n",
        "        return: mean_action, log_prob_action\n",
        "        \"\"\"\n",
        "        # TODO: your code\n",
        "     \n",
        "        x = obs.permute(0,3,1,2)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        mu = F.relu(self.fc_mu(x))\n",
        "        std = F.softplus(self.fc_std(x))\n",
        "        dist = torch.distributions.Normal(mu, std)\n",
        "\n",
        "        action = dist.rsample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Enforcing action bounds\n",
        "        adjusted_action = torch.tanh(action) * self.action_scale + self.action_bias\n",
        "        adjusted_log_prob = log_prob - torch.log(self.action_scale * (1-torch.tanh(action).pow(2)) + 1e-6)\n",
        "        return adjusted_action, adjusted_log_prob\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PXAWU_1icIC5"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size: int):\n",
        "        \"\"\"\n",
        "        Create the replay buffer.\n",
        "\n",
        "        :param max_size: Maximum number of transitions in the buffer.\n",
        "        \"\"\"\n",
        "        self.data = []\n",
        "        self.max_size = max_size\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Adds a new transition to the buffer. When the buffer is full, overwrite the oldest transition.\n",
        "\n",
        "        :param obs: The current observation.\n",
        "        :param action: The action.\n",
        "        :param reward: The reward.\n",
        "        :param next_obs: The next observation.\n",
        "        :param terminated: Whether the episode terminated.\n",
        "        \"\"\"\n",
        "        if len(self.data) < self.max_size:\n",
        "            self.data.append((obs, action, reward, next_obs, terminated))\n",
        "        else:\n",
        "            self.data[self.position] = (obs, action, reward, next_obs, terminated)\n",
        "        self.position = (self.position + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
        "\n",
        "        :param batch_size: The batch size.\n",
        "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
        "        \"\"\"\n",
        "        return [torch.stack(b) for b in zip(*random.choices(self.data, k=batch_size))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "72kgDKe2cIC5"
      },
      "outputs": [],
      "source": [
        "def update_critics(\n",
        "        q1: nn.Module,\n",
        "        q1_target: nn.Module,\n",
        "        q1_optimizer: optim.Optimizer,\n",
        "        q2: nn.Module,\n",
        "        q2_target: nn.Module,\n",
        "        q2_optimizer: optim.Optimizer,\n",
        "        actor_target: nn.Module,\n",
        "        log_ent_coef: torch.Tensor,\n",
        "        gamma: float,\n",
        "        obs: torch.Tensor,\n",
        "        act: torch.Tensor,\n",
        "        rew: torch.Tensor,\n",
        "        next_obs: torch.Tensor,\n",
        "        tm: torch.Tensor,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Update both of SAC's critics for one optimizer step.\n",
        "\n",
        "    :param q1: The first critic network.\n",
        "    :param q1_target: The target first critic network.\n",
        "    :param q1_optimizer: The first critic's optimizer.\n",
        "    :param q2: The second critic network.\n",
        "    :param q2_target: The target second critic network.\n",
        "    :param q2_optimizer: The second critic's optimizer.\n",
        "    :param actor: The actor network.\n",
        "    :param actor_target: The target actor network.\n",
        "    :param actor_optimizer: The actor's optimizer.\n",
        "    :param gamma: The discount factor.\n",
        "    :param obs: Batch of current observations.\n",
        "    :param act: Batch of actions.\n",
        "    :param rew: Batch of rewards.\n",
        "    :param next_obs: Batch of next observations.\n",
        "    :param tm: Batch of termination flags.\n",
        "\n",
        "    \"\"\"\n",
        "    #Calculate the target\n",
        "    with torch.no_grad():\n",
        "        next_action, next_action_log_prob = actor_target(next_obs)\n",
        "        next_action  = next_action.to(device)\n",
        "        next_action_log_prob  = next_action_log_prob.to(device)\n",
        "        q1_tg = q1_target(next_obs,  next_action) \n",
        "        q2_tg = q2_target(next_obs,  next_action)\n",
        "        min_q_target = torch.min(q1_tg, q2_tg) - log_ent_coef.exp() * next_action_log_prob\n",
        "        target_q = rew + gamma * (1 - tm.int()) * min_q_target[:,1]\n",
        "\n",
        "    #Update both q function using our target\n",
        "    for q, optimizer in [(q1, q1_optimizer), (q2, q2_optimizer)]:\n",
        "        critic_loss = F.mse_loss(q(obs, act).squeeze(1), target_q)\n",
        "        optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def update_actor(\n",
        "    q1: nn.Module,\n",
        "    q2: nn.Module,\n",
        "    actor: nn.Module,\n",
        "    actor_optimizer: optim.Optimizer,\n",
        "    obs: torch.Tensor,\n",
        "    log_ent_coef: torch.Tensor,\n",
        "    action: torch.Tensor,\n",
        "    action_log_prob: torch.Tensor,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Update the SAC's Actor network for one optimizer step.\n",
        "\n",
        "    :param critic: The critic network.\n",
        "    :param actor: The actor network.\n",
        "    :param actor_optimizer: The actor's optimizer.\n",
        "    :param obs: Batch of current observations.\n",
        "    :param action: action calculated\n",
        "    :param action_log_prob: log probabilty of action\n",
        "\n",
        "    \"\"\"\n",
        "    # Actor Update\n",
        "\n",
        "   # action , action_log_prob = actor(obs)\n",
        "    entropy =  - log_ent_coef.exp() * action_log_prob\n",
        "    q1, q2 = q1(obs, action), q2(obs, action)\n",
        "    q1_q2 = torch.cat([q1, q2], dim=1)\n",
        "    min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
        "    actor_loss = (- min_q - entropy).mean()\n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "\n",
        "def update_entropy_coefficient(\n",
        "        actor: nn.Module,\n",
        "        log_ent_coef: torch.Tensor,\n",
        "        target_entropy: float,\n",
        "        ent_coef_optimizer: optim.Optimizer,\n",
        "        obs: torch.Tensor,\n",
        "        action_log_prob: torch.Tensor\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Automatic update for entropy coefficient (alpha)\n",
        "\n",
        "    :param actor: the actor network.\n",
        "    :param log_ent_coef: tensor representing the log of entropy coefficient (log_alpha).\n",
        "    :param target_entropy: tensor representing the desired target entropy.\n",
        "    :param ent_coef_optimizer: torch optimizer for entropy coefficient.\n",
        "    :param obs: current batch observation.\n",
        "    :param action_log_prob: log probability\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "   # _, action_log_prob = actor(obs)\n",
        "\n",
        "\n",
        "    ent_coef_loss = -(log_ent_coef.exp() * (action_log_prob + target_entropy).detach()).mean()\n",
        "    ent_coef_optimizer.zero_grad()\n",
        "    ent_coef_loss.backward()\n",
        "    ent_coef_optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X02UoKtocIC6"
      },
      "outputs": [],
      "source": [
        "def polyak_update(\n",
        "    params: Iterable[torch.Tensor],\n",
        "    target_params: Iterable[torch.Tensor],\n",
        "    tau: float,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Perform a Polyak average update on ``target_params`` using ``params``:\n",
        "\n",
        "    :param params: parameters of the original network (model.parameters())\n",
        "    :param target_params: parameters of the target network (model_target.parameters())\n",
        "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) 1 -> Hard update, 0 -> No update\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param, target_param in zip(params, target_params):\n",
        "            target_param.data.mul_(1 - tau)\n",
        "            torch.add(target_param.data, param.data, alpha=tau, out=target_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KqOAA_W4cIC6"
      },
      "outputs": [],
      "source": [
        "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
        "\n",
        "\n",
        "class SACAgent:\n",
        "    def __init__(self,\n",
        "            env,\n",
        "            gamma=0.99,\n",
        "            lr=0.001,\n",
        "            batch_size=64,\n",
        "            tau=0.005,\n",
        "            maxlen=100_000,\n",
        "            target_entropy=-1.0,\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initialize the SAC agent.\n",
        "\n",
        "        :param env: The environment.\n",
        "        :param exploration_noise.\n",
        "        :param gamma: The discount factor.\n",
        "        :param lr: The learning rate.\n",
        "        :param batch_size: Mini batch size.\n",
        "        :param tau: Polyak update coefficient.\n",
        "        :param max_size: Maximum number of transitions in the buffer.\n",
        "        \"\"\"\n",
        "\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.target_entropy=target_entropy\n",
        "\n",
        "        # Initialize the Replay Buffer\n",
        "        self.buffer = ReplayBuffer(maxlen)\n",
        "\n",
        "        # Initialize two critic and one actor network\n",
        "\n",
        "\n",
        "        obs_shape = env.observation_space.shape\n",
        "        print(f'obs shape {obs_shape} ')\n",
        "        act_shape = env.action_space.shape[0]\n",
        "        print(f'actor shape {act_shape} ')\n",
        "\n",
        "        self.q1 = Critic(obs_shape, act_shape).to(device)\n",
        "        self.q2 = Critic(obs_shape, act_shape).to(device)\n",
        "        self.actor = Actor(obs_shape, act_shape, env.action_space.low , env.action_space.high).to(device)\n",
        "        self.log_ent_coef = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "\n",
        "        # Initialze two target critic and one target actor networks and load the corresponding state_dicts\n",
        "        self.q1_target = Critic(obs_shape, act_shape).to(device)\n",
        "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
        "        self.q2_target = Critic(obs_shape, act_shape).to(device)\n",
        "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
        "        self.actor_target =  Actor(obs_shape, act_shape, env.action_space.low , env.action_space.high).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "\n",
        "        # Create ADAM optimizer for the Critic and Actor networks\n",
        "        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=lr)\n",
        "        self.q2_optimizer =  optim.Adam(self.q2.parameters(), lr=lr)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.ent_coef_optimizer = optim.Adam([self.log_ent_coef], lr=lr)\n",
        "\n",
        "        self.log_ent_coef  =  self.log_ent_coef.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, num_episodes: int) -> EpisodeStats:\n",
        "        \"\"\"\n",
        "        Train the SAC agent.\n",
        "\n",
        "        :param num_episodes: Number of episodes to train.\n",
        "        :returns: The episode statistics.\n",
        "        \"\"\"\n",
        "        # Keeps track of useful statistics\n",
        "        stats = EpisodeStats(\n",
        "            episode_lengths=np.zeros(num_episodes),\n",
        "            episode_rewards=np.zeros(num_episodes),\n",
        "        )\n",
        "        current_timestep = 0\n",
        "\n",
        "        for i_episode in range(num_episodes):\n",
        "            # Print out which episode we're on, useful for debugging.\n",
        "            print(f\"episode\", i_episode)\n",
        "            if (i_episode + 1) % 100 == 0:\n",
        "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}')\n",
        "\n",
        "            # Reset the environment and get initial observation\n",
        "            obs, _ = self.env.reset()\n",
        "\n",
        "\n",
        "            obs_batch =  torch.tensor([]).to(device)\n",
        "            rew_batch =  torch.tensor([]).to(device)\n",
        "            tm_batch =  torch.tensor([]).to(device)\n",
        "            act_batch =  torch.tensor([]).to(device)\n",
        "            next_obs_batch =  torch.tensor([]).to(device)\n",
        "\n",
        "            for episode_time in itertools.count():\n",
        "                # Choose action and execute\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    action, _ = self.actor(torch.as_tensor(obs).unsqueeze(0).float().to(device))\n",
        "\n",
        "                    action = action.cpu().numpy().clip(self.env.action_space.low, self.env.action_space.high)\n",
        "                    action = action.squeeze(0)\n",
        "\n",
        "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "                # Update statistics\n",
        "                stats.episode_rewards[i_episode] += reward\n",
        "                stats.episode_lengths[i_episode] += 1\n",
        "\n",
        "                # Store sample in the replay buffer\n",
        "                self.buffer.store(\n",
        "                    torch.as_tensor(obs, dtype=torch.float32),\n",
        "                    torch.as_tensor(action),\n",
        "                    torch.as_tensor(reward, dtype=torch.float32),\n",
        "                    torch.as_tensor(next_obs, dtype=torch.float32),\n",
        "                    torch.as_tensor(terminated),\n",
        "                )\n",
        "\n",
        "                # Sample a mini batch from the replay buffer\n",
        "                obs_batch, act_batch, rew_batch, next_obs_batch, tm_batch = self.buffer.sample(self.batch_size)\n",
        "\n",
        "\n",
        "                obs_batch =  obs_batch.to(device)\n",
        "                rew_batch =  rew_batch.to(device)\n",
        "                tm_batch =  tm_batch.to(device)\n",
        "                act_batch =  act_batch.to(device)\n",
        "                next_obs_batch =  next_obs_batch.to(device)\n",
        "\n",
        "\n",
        "\n",
        "                # Update the Critic network\n",
        "                update_critics(\n",
        "                    self.q1,\n",
        "                    self.q1_target,\n",
        "                    self.q1_optimizer,\n",
        "                    self.q2,\n",
        "                    self.q2_target,\n",
        "                    self.q2_optimizer,\n",
        "                    self.actor_target,\n",
        "                    self.log_ent_coef,\n",
        "                    self.gamma,\n",
        "                    obs_batch,\n",
        "                    act_batch,\n",
        "                    rew_batch,\n",
        "                    next_obs_batch,\n",
        "                    tm_batch\n",
        "                )\n",
        "\n",
        "\n",
        "                action_update , action_log_prob_update = self.actor(obs_batch.float())               #done to avoid double calculation\n",
        "\n",
        "                action_update  = action_update.to(device)\n",
        "                action_log_prob_update  = action_log_prob_update.to(device)\n",
        "\n",
        "                # Update the Actor network\n",
        "                update_actor(\n",
        "                    self.q1,\n",
        "                    self.q2,\n",
        "                    self.actor,\n",
        "                    self.actor_optimizer,\n",
        "                    obs_batch.float(),\n",
        "                    self.log_ent_coef,\n",
        "                    action_update,\n",
        "                    action_log_prob_update,\n",
        "                )\n",
        "\n",
        "                # Update Entropy Coefficient\n",
        "                update_entropy_coefficient(\n",
        "                    self.actor,\n",
        "                    self.log_ent_coef,\n",
        "                    self.target_entropy,\n",
        "                    self.ent_coef_optimizer,\n",
        "                    obs_batch.float(),\n",
        "                    action_log_prob_update,\n",
        "                )\n",
        "\n",
        "                # Update the target networks via Polyak Update\n",
        "                polyak_update(self.q1.parameters(), self.q1_target.parameters(), self.tau)\n",
        "                polyak_update(self.q2.parameters(), self.q2_target.parameters(), self.tau)\n",
        "                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n",
        "\n",
        "                current_timestep += 1\n",
        "\n",
        "                # Check whether the episode is finished\n",
        "                if terminated or truncated or episode_time >= 500:\n",
        "                    break\n",
        "                obs = next_obs\n",
        "        return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fU9HY692cIC6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on CarRacing-v2\n",
            "Observation space: Box(0, 255, (96, 96, 3), uint8)\n",
            "Action space: Box([-1.  0.  0.], 1.0, (3,), float32)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n# Hyperparameters\\nLR = 0.00001                                                        #lowered the lr to improve stability\\nBATCH_SIZE = 32\\n\\nREPLAY_BUFFER_SIZE = 100_000\\nTAU = 0.005\\nNUM_EPISODES = 10\\nDISCOUNT_FACTOR = 0.99\\nTARGET_ENTROPY = -1.0\\n\\n# Train SAC\\nagent = SACAgent(\\n    env,\\n    gamma=DISCOUNT_FACTOR,\\n    lr=LR,\\n    batch_size=BATCH_SIZE,\\n    tau=TAU,\\n    maxlen=REPLAY_BUFFER_SIZE,\\n    target_entropy=TARGET_ENTROPY,\\n)\\nstats = agent.train(NUM_EPISODES)\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Choose your environment\n",
        "env = gym.make(\"CarRacing-v2\",\n",
        "    continuous = True,\n",
        "\n",
        "    render_mode=\"rgb_array\")\n",
        "# Print observation and action space infos\n",
        "print(f\"Training on {env.spec.id}\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\\n\")\n",
        "'''\n",
        "# Hyperparameters\n",
        "LR = 0.00001                                                        #lowered the lr to improve stability\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "REPLAY_BUFFER_SIZE = 100_000\n",
        "TAU = 0.005\n",
        "NUM_EPISODES = 10\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "TARGET_ENTROPY = -1.0\n",
        "\n",
        "# Train SAC\n",
        "agent = SACAgent(\n",
        "    env,\n",
        "    gamma=DISCOUNT_FACTOR,\n",
        "    lr=LR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    tau=TAU,\n",
        "    maxlen=REPLAY_BUFFER_SIZE,\n",
        "    target_entropy=TARGET_ENTROPY,\n",
        ")\n",
        "stats = agent.train(NUM_EPISODES)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-05 14:05:30,862] A new study created in memory with name: no-name-2b2c164b-a7dd-4416-b4d8-cfdacc9c2310\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "obs shape (96, 96, 3) \n",
            "actor shape 3 \n",
            "episode 0\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the hyperparameter search space\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True)\n",
        "    alpha = trial.suggest_float('alpha', -1.0, -0.1)\n",
        "    buffer_size = trial.suggest_int('buffer_size', 1000, 50000)\n",
        "    \n",
        "    # Other hyperparameters and setup\n",
        "    gamma = 0.99\n",
        "    batch_size = 64\n",
        "    max_episodes = 1\n",
        "    tau = 0.005\n",
        "\n",
        "\n",
        "    # Initialize SAC agent with the current hyperparameters\n",
        "    agent = SACAgent(\n",
        "        env,\n",
        "        gamma=gamma,\n",
        "        lr=learning_rate,\n",
        "        batch_size=batch_size,\n",
        "        tau=tau,\n",
        "        maxlen=buffer_size,\n",
        "        target_entropy=alpha,\n",
        "        )\n",
        "        \n",
        "    # Train the agent and get the average return\n",
        "    stats = agent.train(max_episodes)\n",
        "\n",
        "    print(stats)\n",
        "\n",
        "    # Store the trained agent in user attributes\n",
        "    trial.set_user_attr('agent', agent)\n",
        "    trial.set_user_attr('Stats', stats)\n",
        "\n",
        "    trial.report(stats)\n",
        "    \n",
        "    # Handle pruning based on the intermediate value.\n",
        "    if trial.should_prune():\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "    \n",
        "    return stats.episode_rewards.mean()\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction='maximize') \n",
        "\n",
        "# Uncomment the line below to set the storage URL for distributed optimization\n",
        "# study = optuna.create_study(direction='minimize', storage='sqlite:///example.db')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=(optuna.trial.TrialState.PRUNED,))\n",
        "complete_trials = study.get_trials(deepcopy=False, states=(optuna.trial.TrialState.COMPLETE,))\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Best trial:\")\n",
        "\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"Value: \", -trial.value)  # Reverse the sign back to positive\n",
        "print(\"Params: \")\n",
        "\n",
        "best_params = {}\n",
        "\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "    best_params[key] = value\n",
        "\n",
        "stats = trial.user_attrs['Stats'].episode_lengths\n",
        "\n",
        "agent = trial.user_attrs['agent']\n",
        "\n",
        "loaded_actor = agent.actor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jnymfyjcIC7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# save the trained actor\n",
        "torch.save(agent.actor, \"sac_actor.pt\")\n",
        "\n",
        "# loading the trained actor\n",
        "loaded_actor = torch.load(\"sac_actor.pt\")\n",
        "#'''\n",
        "loaded_actor.eval()\n",
        "print(loaded_actor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2psdv32cIC7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image as IImage\n",
        "\n",
        "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
        "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
        "    # Create a list to hold each frame\n",
        "    frames = []\n",
        "\n",
        "    # Convert RGB arrays to PIL Image objects\n",
        "    for rgb_array in rgb_arrays:\n",
        "        rgb_array = (rgb_array).astype(np.uint8)\n",
        "        img = Image.fromarray(rgb_array)\n",
        "        frames.append(img)\n",
        "\n",
        "    # Save the frames as an animated GIF\n",
        "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
        "\n",
        "def rendered_rollout(policy, env, max_steps=10_000):\n",
        "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    imgs = [env.render()]\n",
        "\n",
        "\n",
        "    obs = torch.tensor(obs).to(device)                                                          # Dimensions didnt match so this was added\n",
        "    obs  = obs.unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            action = policy(torch.as_tensor(obs, dtype=torch.float32))[0]                        #.cpu().numpy()  removed process in gpu\n",
        "\n",
        "        action = torch.tensor(action)\n",
        "\n",
        "        action  = action.squeeze(0)\n",
        "        action   = action.to('cpu')\n",
        "\n",
        "        action  = action.numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        obs, _, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "\n",
        "        obs = torch.tensor(obs).to(device)\n",
        "        obs  = obs.unsqueeze(0)\n",
        "\n",
        "\n",
        "        imgs.append(env.render())\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    return imgs\n",
        "\n",
        "imgs = rendered_rollout(loaded_actor, env)\n",
        "save_rgb_animation(imgs, \"trained.gif\")\n",
        "IImage(filename=\"trained.gif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PalXP9CscIC7"
      },
      "outputs": [],
      "source": [
        "smoothing_window=1\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
        "\n",
        "# Plot the episode length over time\n",
        "ax = axes[0]\n",
        "ax.plot(stats.episode_lengths)\n",
        "ax.set_xlabel(\"Episode\")\n",
        "ax.set_ylabel(\"Episode Length\")\n",
        "ax.set_title(\"Episode Length over Time\")\n",
        "\n",
        "# Plot the episode reward over time\n",
        "ax = axes[1]\n",
        "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
        "ax.plot(rewards_smoothed)\n",
        "ax.set_xlabel(\"Episode\")\n",
        "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
        "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
