{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4_NA9Y1cIC0"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install torch\n",
        "!pip install swig\n",
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "!pip install matplotlib\n",
        "!pip install imageio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppu3DNtccIC1"
      },
      "outputs": [],
      "source": [
        "!# Imports\n",
        "from typing import Iterable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from PIL import Image\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Callable\n",
        "from collections import namedtuple\n",
        "import itertools\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzNBit4YTuC1",
        "outputId": "d1b4017e-f2ec-4176-cbea-1adebaeab271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available!\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU Available!\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"USE CPU\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VlP3XLDvcIC3"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim: int, action_dim: int):\n",
        "        \"\"\"\n",
        "        Initialize the Critic network.\n",
        "\n",
        "        :param obs_dim: dimention of the observations\n",
        "        :param num_actions: dimention of the actions\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3  , 32 , 3 ,  1 ,1 )\n",
        "        self.conv2 = nn.Conv2d(32 , 64, 3 ,  1 ,1 )\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 24 *24  + action_dim , 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = obs.permute(0,3, 1, 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "\n",
        "\n",
        "        flatx = nn.Flatten()\n",
        "        x = flatx(x)\n",
        "\n",
        "        x = torch.cat([x, action], dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        out = x\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2j5bfzwtcIC4"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim: int, action_dim: int, action_low: np.array, action_high: np.array):\n",
        "        \"\"\"\n",
        "        Initialize the Actor network.\n",
        "\n",
        "        :param obs_dim: dimention of the observations\n",
        "        :param num_actions: dimention of the actions\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        # We are registering scale and bias as buffers so they can be saved and loaded as part of the model.\n",
        "        # Buffers won't be passed to the optimizer for training!\n",
        "        self.register_buffer(\n",
        "            \"action_scale\", torch.tensor((action_high - action_low) / 2.0, dtype=torch.float32)\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"action_bias\", torch.tensor((action_high + action_low) / 2.0, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "        '''\n",
        "        self.conv1 = nn.Conv2d(3  , 64, 3 ,  1 ,1 )\n",
        "        self.conv2 = nn.Conv2d(64 , 128, 3 ,  1 ,1)\n",
        "        self.conv3 = nn.Conv2d(128 , 256, 3 ,  1 ,1 )\n",
        "\n",
        "        self.fc1 = nn.Linear(256 * 6 * 6 * 4 , 256)\n",
        "        self.fc_mu = nn.Linear(256, action_dim)\n",
        "        self.fc_std = nn.Linear(256, action_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        '''\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3  , 32 , 3 ,  1 ,1 )\n",
        "        self.conv2 = nn.Conv2d(32 , 64, 3 ,  1 , 1 )\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 24 * 24  , 256)\n",
        "        self.fc_mu = nn.Linear(256, action_dim)\n",
        "        self.fc_std = nn.Linear(256, action_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the actor network.\n",
        "\n",
        "        return: mean_action, log_prob_action\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        x = obs.permute(0,3,1,2)\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "\n",
        "\n",
        "        flatx = nn.Flatten()\n",
        "        x = flatx(x)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        mu = F.relu(self.fc_mu(x))\n",
        "        std = F.softplus(self.fc_std(x))\n",
        "\n",
        "\n",
        "        dist = torch.distributions.Normal(mu, std)\n",
        "\n",
        "        action = dist.rsample()\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "\n",
        "        # Enforcing action bounds\n",
        "        adjusted_action = torch.tanh(action) * self.action_scale + self.action_bias\n",
        "        adjusted_log_prob = log_prob - torch.log(self.action_scale * (1-torch.tanh(action).pow(2)) + 1e-6)\n",
        "\n",
        "        return adjusted_action, adjusted_log_prob\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PXAWU_1icIC5"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size: int):\n",
        "        \"\"\"\n",
        "        Create the replay buffer.\n",
        "\n",
        "        :param max_size: Maximum number of transitions in the buffer.\n",
        "        \"\"\"\n",
        "        self.data = []\n",
        "        self.max_size = max_size\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Adds a new transition to the buffer. When the buffer is full, overwrite the oldest transition.\n",
        "\n",
        "        :param obs: The current observation.\n",
        "        :param action: The action.\n",
        "        :param reward: The reward.\n",
        "        :param next_obs: The next observation.\n",
        "        :param terminated: Whether the episode terminated.\n",
        "        \"\"\"\n",
        "        if len(self.data) < self.max_size:\n",
        "            self.data.append((obs, action, reward, next_obs, terminated))\n",
        "        else:\n",
        "            self.data[self.position] = (obs, action, reward, next_obs, terminated)\n",
        "        self.position = (self.position + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
        "\n",
        "        :param batch_size: The batch size.\n",
        "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
        "        \"\"\"\n",
        "        return [torch.stack(b) for b in zip(*random.choices(self.data, k=batch_size))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "72kgDKe2cIC5"
      },
      "outputs": [],
      "source": [
        "def update_critics(\n",
        "        q1: nn.Module,\n",
        "        q1_target: nn.Module,\n",
        "        q1_optimizer: optim.Optimizer,\n",
        "        q2: nn.Module,\n",
        "        q2_target: nn.Module,\n",
        "        q2_optimizer: optim.Optimizer,\n",
        "        actor_target: nn.Module,\n",
        "        log_ent_coef: torch.Tensor,\n",
        "        gamma: float,\n",
        "        obs: torch.Tensor,\n",
        "        act: torch.Tensor,\n",
        "        rew: torch.Tensor,\n",
        "        next_obs: torch.Tensor,\n",
        "        tm: torch.Tensor,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Update both of SAC's critics for one optimizer step.\n",
        "\n",
        "    :param q1: The first critic network.\n",
        "    :param q1_target: The target first critic network.\n",
        "    :param q1_optimizer: The first critic's optimizer.\n",
        "    :param q2: The second critic network.\n",
        "    :param q2_target: The target second critic network.\n",
        "    :param q2_optimizer: The second critic's optimizer.\n",
        "    :param actor: The actor network.\n",
        "    :param actor_target: The target actor network.\n",
        "    :param actor_optimizer: The actor's optimizer.\n",
        "    :param gamma: The discount factor.\n",
        "    :param obs: Batch of current observations.\n",
        "    :param act: Batch of actions.\n",
        "    :param rew: Batch of rewards.\n",
        "    :param next_obs: Batch of next observations.\n",
        "    :param tm: Batch of termination flags.\n",
        "\n",
        "    \"\"\"\n",
        "    #Calculate the target\n",
        "    with torch.no_grad():\n",
        "         act_target  =  actor_target(next_obs)[0]\n",
        "         act_target  = act_target.to(device)\n",
        "         target_q  = rew   +  (1 - tm.float())* gamma * (torch.min(q2_target(next_obs,  act_target).squeeze(1)  , q1_target(next_obs,  act_target).squeeze(1)) )\n",
        "         target_q = target_q.to(device)\n",
        "    #Update both q function using our target\n",
        "    for q, optimizer in [(q1, q1_optimizer), (q2, q2_optimizer)]:\n",
        "\n",
        "        critic_loss =  F.mse_loss(  q1(obs , act).squeeze(1) , target_q)   +   F.mse_loss(  q2(obs , act).squeeze(1) , target_q)\n",
        "        optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def update_actor(\n",
        "    q1: nn.Module,\n",
        "    q2: nn.Module,\n",
        "    actor: nn.Module,\n",
        "    actor_optimizer: optim.Optimizer,\n",
        "    obs: torch.Tensor,\n",
        "    log_ent_coef: torch.Tensor,\n",
        "    action: torch.Tensor,\n",
        "    action_log_prob: torch.Tensor,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Update the SAC's Actor network for one optimizer step.\n",
        "\n",
        "    :param critic: The critic network.\n",
        "    :param actor: The actor network.\n",
        "    :param actor_optimizer: The actor's optimizer.\n",
        "    :param obs: Batch of current observations.\n",
        "    :param action: action calculated\n",
        "    :param action_log_prob: log probabilty of action\n",
        "\n",
        "    \"\"\"\n",
        "    # Actor Update\n",
        "\n",
        "   # action , action_log_prob = actor(obs)\n",
        "    entropy =  - log_ent_coef.exp() * action_log_prob\n",
        "    q1, q2 = q1(obs, action), q2(obs, action)\n",
        "    q1_q2 = torch.cat([q1, q2], dim=1)\n",
        "    min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
        "    actor_loss = (- min_q - entropy).mean()\n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "\n",
        "def update_entropy_coefficient(\n",
        "        actor: nn.Module,\n",
        "        log_ent_coef: torch.Tensor,\n",
        "        target_entropy: float,\n",
        "        ent_coef_optimizer: optim.Optimizer,\n",
        "        obs: torch.Tensor,\n",
        "        action_log_prob: torch.Tensor\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Automatic update for entropy coefficient (alpha)\n",
        "\n",
        "    :param actor: the actor network.\n",
        "    :param log_ent_coef: tensor representing the log of entropy coefficient (log_alpha).\n",
        "    :param target_entropy: tensor representing the desired target entropy.\n",
        "    :param ent_coef_optimizer: torch optimizer for entropy coefficient.\n",
        "    :param obs: current batch observation.\n",
        "    :param action_log_prob: log probability\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "   # _, action_log_prob = actor(obs)\n",
        "\n",
        "\n",
        "    ent_coef_loss = -(log_ent_coef.exp() * (action_log_prob + target_entropy).detach()).mean()\n",
        "    ent_coef_optimizer.zero_grad()\n",
        "    ent_coef_loss.backward()\n",
        "    ent_coef_optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X02UoKtocIC6"
      },
      "outputs": [],
      "source": [
        "def polyak_update(\n",
        "    params: Iterable[torch.Tensor],\n",
        "    target_params: Iterable[torch.Tensor],\n",
        "    tau: float,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Perform a Polyak average update on ``target_params`` using ``params``:\n",
        "\n",
        "    :param params: parameters of the original network (model.parameters())\n",
        "    :param target_params: parameters of the target network (model_target.parameters())\n",
        "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) 1 -> Hard update, 0 -> No update\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param, target_param in zip(params, target_params):\n",
        "            target_param.data.mul_(1 - tau)\n",
        "            torch.add(target_param.data, param.data, alpha=tau, out=target_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KqOAA_W4cIC6"
      },
      "outputs": [],
      "source": [
        "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
        "\n",
        "\n",
        "class SACAgent:\n",
        "    def __init__(self,\n",
        "            env,\n",
        "            gamma=0.99,\n",
        "            lr=0.001,\n",
        "            batch_size=64,\n",
        "            tau=0.005,\n",
        "            maxlen=100_000,\n",
        "            target_entropy=-1.0,\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initialize the SAC agent.\n",
        "\n",
        "        :param env: The environment.\n",
        "        :param exploration_noise.\n",
        "        :param gamma: The discount factor.\n",
        "        :param lr: The learning rate.\n",
        "        :param batch_size: Mini batch size.\n",
        "        :param tau: Polyak update coefficient.\n",
        "        :param max_size: Maximum number of transitions in the buffer.\n",
        "        \"\"\"\n",
        "\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.target_entropy=target_entropy\n",
        "\n",
        "        # Initialize the Replay Buffer\n",
        "        self.buffer = ReplayBuffer(maxlen)\n",
        "\n",
        "        # Initialize two critic and one actor network\n",
        "\n",
        "\n",
        "        obs_shape = int(env.observation_space.shape[0])\n",
        "        print(f'obs shape {obs_shape} ')\n",
        "        act_shape = int(env.action_space.shape[0])\n",
        "        print(f'actor shape {act_shape} ')\n",
        "\n",
        "        self.q1 = Critic(obs_shape, act_shape).to(device)\n",
        "        self.q2 = Critic(obs_shape, act_shape).to(device)\n",
        "        self.actor = Actor(obs_shape, act_shape, env.action_space.low , env.action_space.high).to(device)\n",
        "        self.log_ent_coef = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "\n",
        "        # Initialze two target critic and one target actor networks and load the corresponding state_dicts\n",
        "        self.q1_target = Critic(obs_shape, act_shape).to(device)\n",
        "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
        "        self.q2_target = Critic(obs_shape, act_shape).to(device)\n",
        "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
        "        self.actor_target =  Actor(obs_shape, act_shape, env.action_space.low , env.action_space.high).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "\n",
        "        # Create ADAM optimizer for the Critic and Actor networks\n",
        "        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=lr)\n",
        "        self.q2_optimizer =  optim.Adam(self.q2.parameters(), lr=lr)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.ent_coef_optimizer = optim.Adam([self.log_ent_coef], lr=lr)\n",
        "\n",
        "        self.log_ent_coef  =  self.log_ent_coef.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, num_episodes: int) -> EpisodeStats:\n",
        "        \"\"\"\n",
        "        Train the SAC agent.\n",
        "\n",
        "        :param num_episodes: Number of episodes to train.\n",
        "        :returns: The episode statistics.\n",
        "        \"\"\"\n",
        "        # Keeps track of useful statistics\n",
        "        stats = EpisodeStats(\n",
        "            episode_lengths=np.zeros(num_episodes),\n",
        "            episode_rewards=np.zeros(num_episodes),\n",
        "        )\n",
        "        current_timestep = 0\n",
        "\n",
        "        for i_episode in range(num_episodes):\n",
        "            # Print out which episode we're on, useful for debugging.\n",
        "            print(f\"episode\", i_episode)\n",
        "            if (i_episode + 1) % 100 == 0:\n",
        "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}')\n",
        "\n",
        "            # Reset the environment and get initial observation\n",
        "            obs, _ = self.env.reset()\n",
        "\n",
        "\n",
        "            obs_batch =  torch.tensor([]).to(device)\n",
        "            rew_batch =  torch.tensor([]).to(device)\n",
        "            tm_batch =  torch.tensor([]).to(device)\n",
        "            act_batch =  torch.tensor([]).to(device)\n",
        "            next_obs_batch =  torch.tensor([]).to(device)\n",
        "\n",
        "            for episode_time in itertools.count():\n",
        "                # Choose action and execute\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    action, _ = self.actor(torch.as_tensor(obs).unsqueeze(0).float().to(device))\n",
        "\n",
        "                    action = action.cpu().numpy().clip(self.env.action_space.low, self.env.action_space.high)\n",
        "                    action = action.squeeze(0)\n",
        "\n",
        "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "                # Update statistics\n",
        "                stats.episode_rewards[i_episode] += reward\n",
        "                stats.episode_lengths[i_episode] += 1\n",
        "\n",
        "                # Store sample in the replay buffer\n",
        "                self.buffer.store(\n",
        "                    torch.as_tensor(obs, dtype=torch.float32),\n",
        "                    torch.as_tensor(action),\n",
        "                    torch.as_tensor(reward, dtype=torch.float32),\n",
        "                    torch.as_tensor(next_obs, dtype=torch.float32),\n",
        "                    torch.as_tensor(terminated),\n",
        "                )\n",
        "\n",
        "                # Sample a mini batch from the replay buffer\n",
        "                obs_batch, act_batch, rew_batch, next_obs_batch, tm_batch = self.buffer.sample(self.batch_size)\n",
        "\n",
        "\n",
        "                obs_batch =  obs_batch.to(device)\n",
        "                rew_batch =  rew_batch.to(device)\n",
        "                tm_batch =  tm_batch.to(device)\n",
        "                act_batch =  act_batch.to(device)\n",
        "                next_obs_batch =  next_obs_batch.to(device)\n",
        "\n",
        "\n",
        "\n",
        "                # Update the Critic network\n",
        "                update_critics(\n",
        "                    self.q1,\n",
        "                    self.q1_target,\n",
        "                    self.q1_optimizer,\n",
        "                    self.q2,\n",
        "                    self.q2_target,\n",
        "                    self.q2_optimizer,\n",
        "                    self.actor_target,\n",
        "                    self.log_ent_coef,\n",
        "                    self.gamma,\n",
        "                    obs_batch,\n",
        "                    act_batch,\n",
        "                    rew_batch,\n",
        "                    next_obs_batch,\n",
        "                    tm_batch\n",
        "                )\n",
        "\n",
        "\n",
        "                action_update , action_log_prob_update = self.actor(obs_batch.float())               #done to avoid double calculation\n",
        "\n",
        "                action_update  = action_update.to(device)\n",
        "                action_log_prob_update  = action_log_prob_update.to(device)\n",
        "\n",
        "                # Update the Actor network\n",
        "                update_actor(\n",
        "                    self.q1,\n",
        "                    self.q2,\n",
        "                    self.actor,\n",
        "                    self.actor_optimizer,\n",
        "                    obs_batch.float(),\n",
        "                    self.log_ent_coef,\n",
        "                    action_update,\n",
        "                    action_log_prob_update,\n",
        "                )\n",
        "\n",
        "                # Update Entropy Coefficient\n",
        "                update_entropy_coefficient(\n",
        "                    self.actor,\n",
        "                    self.log_ent_coef,\n",
        "                    self.target_entropy,\n",
        "                    self.ent_coef_optimizer,\n",
        "                    obs_batch.float(),\n",
        "                    action_log_prob_update,\n",
        "                )\n",
        "\n",
        "                # Update the target networks via Polyak Update\n",
        "                polyak_update(self.q1.parameters(), self.q1_target.parameters(), self.tau)\n",
        "                polyak_update(self.q2.parameters(), self.q2_target.parameters(), self.tau)\n",
        "                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n",
        "\n",
        "                current_timestep += 1\n",
        "\n",
        "                # Check whether the episode is finished\n",
        "                if terminated or truncated or episode_time >= 500:\n",
        "                    break\n",
        "                obs = next_obs\n",
        "        return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU9HY692cIC6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Choose your environment\n",
        "env = gym.make(\"CarRacing-v2\",\n",
        "    continuous = True,\n",
        "\n",
        "    render_mode=\"rgb_array\")\n",
        "# Print observation and action space infos\n",
        "print(f\"Training on {env.spec.id}\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "LR = 0.00001                                                        #lowered the lr to improve stability\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "REPLAY_BUFFER_SIZE = 100_000\n",
        "TAU = 0.005\n",
        "NUM_EPISODES = 100\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "TARGET_ENTROPY = -1.0\n",
        "\n",
        "# Train SAC\n",
        "agent = SACAgent(\n",
        "    env,\n",
        "    gamma=DISCOUNT_FACTOR,\n",
        "    lr=LR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    tau=TAU,\n",
        "    maxlen=REPLAY_BUFFER_SIZE,\n",
        "    target_entropy=TARGET_ENTROPY,\n",
        ")\n",
        "stats = agent.train(NUM_EPISODES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jnymfyjcIC7"
      },
      "outputs": [],
      "source": [
        "# save the trained actor\n",
        "torch.save(agent.actor, \"sac_actor.pt\")\n",
        "\n",
        "# loading the trained actor\n",
        "loaded_actor = torch.load(\"sac_actor.pt\")\n",
        "loaded_actor.eval()\n",
        "print(loaded_actor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2psdv32cIC7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image as IImage\n",
        "\n",
        "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
        "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
        "    # Create a list to hold each frame\n",
        "    frames = []\n",
        "\n",
        "    # Convert RGB arrays to PIL Image objects\n",
        "    for rgb_array in rgb_arrays:\n",
        "        rgb_array = (rgb_array).astype(np.uint8)\n",
        "        img = Image.fromarray(rgb_array)\n",
        "        frames.append(img)\n",
        "\n",
        "    # Save the frames as an animated GIF\n",
        "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
        "\n",
        "def rendered_rollout(policy, env, max_steps=10_000):\n",
        "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    imgs = [env.render()]\n",
        "\n",
        "\n",
        "    obs = torch.tensor(obs).to(device)                                                          # Dimensions didnt match so this was added\n",
        "    obs  = obs.unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            action = policy(torch.as_tensor(obs, dtype=torch.float32))[0]                        #.cpu().numpy()  removed process in gpu\n",
        "\n",
        "        action = torch.tensor(action)\n",
        "\n",
        "        action  = action.squeeze(0)\n",
        "        action   = action.to('cpu')\n",
        "\n",
        "        action  = action.numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        obs, _, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "\n",
        "        obs = torch.tensor(obs).to(device)\n",
        "        obs  = obs.unsqueeze(0)\n",
        "\n",
        "\n",
        "        imgs.append(env.render())\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    return imgs\n",
        "\n",
        "imgs = rendered_rollout(loaded_actor, env)\n",
        "save_rgb_animation(imgs, \"trained.gif\")\n",
        "IImage(filename=\"trained.gif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PalXP9CscIC7"
      },
      "outputs": [],
      "source": [
        "smoothing_window=20\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
        "\n",
        "# Plot the episode length over time\n",
        "ax = axes[0]\n",
        "ax.plot(stats.episode_lengths)\n",
        "ax.set_xlabel(\"Episode\")\n",
        "ax.set_ylabel(\"Episode Length\")\n",
        "ax.set_title(\"Episode Length over Time\")\n",
        "\n",
        "# Plot the episode reward over time\n",
        "ax = axes[1]\n",
        "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
        "ax.plot(rewards_smoothed)\n",
        "ax.set_xlabel(\"Episode\")\n",
        "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
        "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}