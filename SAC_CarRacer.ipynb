{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w17R0qWZcICw"
      },
      "source": [
        "# Exercise 5 - SAC\n",
        "In this last exercise we will train our own agent using [Soft Actor Critic](https://arxiv.org/pdf/1801.01290.pdf). Currently it is one of the state-of-the-art methods for continuous control with similarities to DDPG and TD3 from the last exercise. Once again we will solve the LunarLander environment from Gymnasium to safely land our vehicle on the surface on the moon.\n",
        "\n",
        "Note: A good introduction to SAC can be found at the [Spinning Up project](https://spinningup.openai.com/en/latest/algorithms/sac.html).\n",
        "\n",
        "<img src=\"resources/lunar_lander.gif\" alt=\"Gymnasium\" width=\"20%\"/>\n",
        "\n",
        "<!--  -->\n",
        "_Agent using random actions to land the lunarlander_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M0ozV9QcIC0"
      },
      "source": [
        "## 0 Setup\n",
        "These are the same packages as in the last exercise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A4_NA9Y1cIC0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (2.1.1)\n",
            "Requirement already satisfied: filelock in /home/fabian/.local/lib/python3.9/site-packages (from torch) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: sympy in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/fabian/.local/lib/python3.9/site-packages (from torch) (2.6.2)\n",
            "Requirement already satisfied: jinja2 in /home/fabian/.local/lib/python3.9/site-packages (from torch) (3.0.1)\n",
            "Requirement already satisfied: fsspec in /home/fabian/.local/lib/python3.9/site-packages (from torch) (2021.8.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/fabian/.local/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: swig in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (4.1.1.post1)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gymnasium in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium) (1.26.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/fabian/.local/lib/python3.9/site-packages (from gymnasium) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium) (4.8.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium) (6.8.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gymnasium[box2d] in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (1.26.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/fabian/.local/lib/python3.9/site-packages (from gymnasium[box2d]) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (4.8.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (6.8.0)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium[box2d]) (4.1.1.post1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.17.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: matplotlib in /home/fabian/.local/lib/python3.9/site-packages (3.3.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/fabian/.local/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/fabian/.local/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (1.26.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (10.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/fabian/.local/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: imageio in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (2.33.0)\n",
            "Requirement already satisfied: numpy in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from imageio) (1.26.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from imageio) (10.1.0)\n",
            "\u001b[33mDEPRECATION: markdown 3.3.5 has a non-standard dependency specifier importlib-metadata>='4.4'; python_version < \"3.10\". pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of markdown or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install torch\n",
        "!pip install swig\n",
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "!pip install matplotlib\n",
        "!pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ppu3DNtccIC1"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from typing import Iterable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from PIL import Image\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Callable\n",
        "from collections import namedtuple\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USE CPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU Available!\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"USE CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0vEaZ4FcIC2"
      },
      "source": [
        "## 1 SAC\n",
        "Soft-Actor Critic (SAC) is an off-policy actor-critic RL method, which maximizes an objective with **entropy regularization**: $J = J_θ + α H(π_θ)$, where $J_θ$ is the expected return with respect to the parameters $\\theta$ of our policy, which we typically maximize, $H$ is the entropy and $\\alpha$ is a hyperparameter to balance it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1fKO886cIC3"
      },
      "source": [
        "## 1.1 Critic\n",
        "We reuse the Critic implementation from last exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VlP3XLDvcIC3"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim: torch.Size, action_dim: torch.Size):\n",
        "        \"\"\"\n",
        "        Initialize the Critic network.\n",
        "\n",
        "        :param obs_dim: dimention of the observations\n",
        "        :param num_actions: dimention of the actions\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(obs_dim[-1], 16, 5, stride=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1)\n",
        "        self.fc1 = nn.Linear(32*(obs_dim[0]-6)*(obs_dim[1]-6) + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        x = obs.permute(0, 3, 1, 2)\n",
        "        \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.cat([x, action], dim=-1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        out = self.fc3(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2iA4-x2cIC4"
      },
      "source": [
        "## 1.2 Actor\n",
        "The actor is also explicitly represented by a neural network. In contrast to DDPG, SAC uses a stochastic policy. This means that the actor outputs a probability distribution over the actions. The distribution is parameterized by a mean $\\mu$ and a standard deviation $\\sigma$. Typically, the stochastic policy is modeled with a squashed diagonal Gaussian distribution. The squashing function is used to ensure that the actions are within the bounds of the environment's action space.\n",
        "\n",
        "### Reparameterization Trick\n",
        "The reason for using a stochastic policy is to allow exploration. However, the stochasticity (sampling directly from $\\mathcal{N}(\\mu(s), \\sigma(s))$) makes it difficult to compute the gradients of the loss function. This is where the reparameterization trick comes in. The trick is to sample the action from a deterministic distribution ($\\mu(s)$ and $\\sigma(s)$) and then add noise ($\\mathcal{N}(0, \\mathcal{I})$). This way, the gradients can be computed with respect to the deterministic distribution, while the actions are still sampled from the stochastic distribution.\n",
        "\n",
        "$$\n",
        "a \\sim \\mathcal{N}(\\mu(s), \\sigma(s)) => \\mu(s) + \\sigma(s) \\cdot \\mathcal{N}(0, \\mathcal{I})\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2j5bfzwtcIC4"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim: torch.Size, action_dim: torch.Size, action_low: np.array, action_high: np.array):\n",
        "        \"\"\"\n",
        "        Initialize the Actor network.\n",
        "\n",
        "        :param obs_dim: dimention of the observations\n",
        "        :param num_actions: dimention of the actions\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        # We are registering scale and bias as buffers so they can be saved and loaded as part of the model.\n",
        "        # Buffers won't be passed to the optimizer for training!\n",
        "        self.register_buffer(\n",
        "            \"action_scale\", torch.tensor((action_high - action_low) / 2.0, dtype=torch.float32)\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"action_bias\", torch.tensor((action_high + action_low) / 2.0, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "        # TODO: your code\n",
        "        self.conv1 = nn.Conv2d(obs_dim[-1], 16, 5, stride=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1)\n",
        "        self.fc1 = nn.Linear(32*(obs_dim[0]-6)*(obs_dim[1]-6), 256)\n",
        "        self.fc_mu = nn.Linear(256, action_dim)\n",
        "        self.fc_std = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass of the actor network.\n",
        "\n",
        "        return: mean_action, log_prob_action\n",
        "        \"\"\"\n",
        "        # TODO: your code\n",
        "     \n",
        "        x = obs.permute(0,3,1,2)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        mu = F.relu(self.fc_mu(x))\n",
        "        std = F.softplus(self.fc_std(x))\n",
        "        dist = torch.distributions.Normal(mu, std)\n",
        "\n",
        "        action = dist.rsample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Enforcing action bounds\n",
        "        adjusted_action = torch.tanh(action) * self.action_scale + self.action_bias\n",
        "        adjusted_log_prob = log_prob - torch.log(self.action_scale * (1-torch.tanh(action).pow(2)) + 1e-6)\n",
        "        return adjusted_action, adjusted_log_prob\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDq02z0QcIC4"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "We reuse the implementation of our replay buffer from last exercise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PXAWU_1icIC5"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size: int):\n",
        "        \"\"\"\n",
        "        Create the replay buffer.\n",
        "\n",
        "        :param max_size: Maximum number of transitions in the buffer.\n",
        "        \"\"\"\n",
        "        self.data = []\n",
        "        self.max_size = max_size\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Adds a new transition to the buffer. When the buffer is full, overwrite the oldest transition.\n",
        "\n",
        "        :param obs: The current observation.\n",
        "        :param action: The action.\n",
        "        :param reward: The reward.\n",
        "        :param next_obs: The next observation.\n",
        "        :param terminated: Whether the episode terminated.\n",
        "        \"\"\"\n",
        "        if len(self.data) < self.max_size:\n",
        "            self.data.append((obs, action, reward, next_obs, terminated))\n",
        "        else:\n",
        "            self.data[self.position] = (obs, action, reward, next_obs, terminated)\n",
        "        self.position = (self.position + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
        "\n",
        "        :param batch_size: The batch size.\n",
        "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
        "        \"\"\"\n",
        "        return [torch.stack(b) for b in zip(*random.choices(self.data, k=batch_size))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKAebfecIC5"
      },
      "source": [
        "## 1.3 Algorithm\n",
        "In this section, we will first look at the update formulas of SAC and then implement the entire algorithm.\n",
        "\n",
        "---\n",
        "**Updating the critics**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "72kgDKe2cIC5"
      },
      "outputs": [],
      "source": [
        "def update_critics(\n",
        "        q1: nn.Module,\n",
        "        q1_target: nn.Module,\n",
        "        q1_optimizer: optim.Optimizer,\n",
        "        q2: nn.Module,\n",
        "        q2_target: nn.Module,\n",
        "        q2_optimizer: optim.Optimizer,\n",
        "        actor_target: nn.Module,\n",
        "        log_ent_coef: torch.Tensor,\n",
        "        gamma: float,\n",
        "        obs: torch.Tensor,\n",
        "        act: torch.Tensor,\n",
        "        rew: torch.Tensor,\n",
        "        next_obs: torch.Tensor,\n",
        "        tm: torch.Tensor,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Update both of SAC's critics for one optimizer step.\n",
        "\n",
        "    :param q1: The first critic network.\n",
        "    :param q1_target: The target first critic network.\n",
        "    :param q1_optimizer: The first critic's optimizer.\n",
        "    :param q2: The second critic network.\n",
        "    :param q2_target: The target second critic network.\n",
        "    :param q2_optimizer: The second critic's optimizer.\n",
        "    :param actor: The actor network.\n",
        "    :param actor_target: The target actor network.\n",
        "    :param actor_optimizer: The actor's optimizer.\n",
        "    :param gamma: The discount factor.\n",
        "    :param obs: Batch of current observations.\n",
        "    :param act: Batch of actions.\n",
        "    :param rew: Batch of rewards.\n",
        "    :param next_obs: Batch of next observations.\n",
        "    :param tm: Batch of termination flags.\n",
        "\n",
        "    \"\"\"\n",
        "    # TODO: 1. Calculate the target\n",
        "    with torch.no_grad():\n",
        "       next_action, next_action_log_prob = actor_target(next_obs)\n",
        "       q1_tg = q1_target(next_obs,  next_action) \n",
        "       q2_tg = q2_target(next_obs,  next_action)\n",
        "       #target_q  = rew   +  (1 - tm.float())* gamma * (torch.min( q2_tg  , q1_tg))\n",
        "       min_q_target = torch.min(q1_tg, q2_tg) - log_ent_coef.exp() * next_action_log_prob\n",
        "       target_q = rew + gamma * (1 - tm.int()) * min_q_target[:,1]\n",
        "\n",
        "    # TODO: 2. Update both q function using our target\n",
        "    for q, optimizer in [(q1, q1_optimizer), (q2, q2_optimizer)]:\n",
        "        #critic_loss =  F.mse_loss(q1(obs , act).squeeze(1), target_q)   +   F.mse_loss(q2(obs , act).squeeze(1) , target_q)\n",
        "        critic_loss = F.mse_loss(q(obs, act).squeeze(1), target_q)\n",
        "        optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def update_actor(\n",
        "    q1: nn.Module,\n",
        "    q2: nn.Module,\n",
        "    actor: nn.Module,\n",
        "    actor_optimizer: optim.Optimizer,\n",
        "    obs: torch.Tensor,\n",
        "    log_ent_coef: torch.Tensor,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Update the SAC's Actor network for one optimizer step.\n",
        "\n",
        "    :param critic: The critic network.\n",
        "    :param actor: The actor network.\n",
        "    :param actor_optimizer: The actor's optimizer.\n",
        "    :param obs: Batch of current observations.\n",
        "\n",
        "    \"\"\"\n",
        "    # Actor Update\n",
        "    action, action_log_prob = actor(obs)\n",
        "    entropy = - log_ent_coef.exp() * action_log_prob\n",
        "    q1, q2 = q1(obs, action), q2(obs, action)\n",
        "    q1_q2 = torch.cat([q1, q2], dim=1)\n",
        "    min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
        "    actor_loss = (- min_q - entropy).mean()\n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "\n",
        "def update_entropy_coefficient(\n",
        "        actor: nn.Module,\n",
        "        log_ent_coef: torch.Tensor,\n",
        "        target_entropy: float,\n",
        "        ent_coef_optimizer: optim.Optimizer,\n",
        "        obs: torch.Tensor,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Automatic update for entropy coefficient (alpha)\n",
        "\n",
        "    :param actor: the actor network.\n",
        "    :param log_ent_coef: tensor representing the log of entropy coefficient (log_alpha).\n",
        "    :param target_entropy: tensor representing the desired target entropy.\n",
        "    :param ent_coef_optimizer: torch optimizer for entropy coefficient.\n",
        "    :param obs: current batch observation.\n",
        "    \"\"\"\n",
        "    _, action_log_prob = actor(obs)\n",
        "    ent_coef_loss = -(log_ent_coef.exp() * (action_log_prob + target_entropy).detach()).mean()\n",
        "    ent_coef_optimizer.zero_grad()\n",
        "    ent_coef_loss.backward()\n",
        "    ent_coef_optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0D2DLMgcIC5"
      },
      "source": [
        "## 1.4 Polyak Update of the target networks\n",
        "It is common to update the target networks very slowly at every step. We use the polyak update from last exercise again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X02UoKtocIC6"
      },
      "outputs": [],
      "source": [
        "def polyak_update(\n",
        "    params: Iterable[torch.Tensor],\n",
        "    target_params: Iterable[torch.Tensor],\n",
        "    tau: float,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Perform a Polyak average update on ``target_params`` using ``params``:\n",
        "\n",
        "    :param params: parameters of the original network (model.parameters())\n",
        "    :param target_params: parameters of the target network (model_target.parameters())\n",
        "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) 1 -> Hard update, 0 -> No update\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param, target_param in zip(params, target_params):\n",
        "            target_param.data.mul_(1 - tau)\n",
        "            torch.add(target_param.data, param.data, alpha=tau, out=target_param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g__H5L8cIC6"
      },
      "source": [
        "**Implementing the SAC Agent**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KqOAA_W4cIC6"
      },
      "outputs": [],
      "source": [
        "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
        "\n",
        "\n",
        "class SACAgent:\n",
        "    def __init__(self,\n",
        "            env,\n",
        "            gamma=0.99,\n",
        "            lr=0.001,\n",
        "            batch_size=64,\n",
        "            tau=0.005,\n",
        "            maxlen=100_000,\n",
        "            target_entropy=-1.0,\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initialize the SAC agent.\n",
        "\n",
        "        :param env: The environment.\n",
        "        :param exploration_noise.\n",
        "        :param gamma: The discount factor.\n",
        "        :param lr: The learning rate.\n",
        "        :param batch_size: Mini batch size.\n",
        "        :param tau: Polyak update coefficient.\n",
        "        :param max_size: Maximum number of transitions in the buffer.\n",
        "        \"\"\"\n",
        "\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.target_entropy=target_entropy\n",
        "\n",
        "        # Initialize the Replay Buffer\n",
        "        self.buffer = ReplayBuffer(maxlen)\n",
        "\n",
        "        # TODO: Initialize two critic and one actor network\n",
        "\n",
        "\n",
        "        obs_shape = env.observation_space.shape\n",
        "        act_shape = env.action_space.shape[0]\n",
        "\n",
        "        self.q1 = Critic(obs_shape, act_shape).to(device)\n",
        "        self.q2 = Critic(obs_shape, act_shape).to(device)\n",
        "        self.actor = Actor(obs_shape, act_shape, env.action_space.low , env.action_space.high).to(device)\n",
        "        self.log_ent_coef = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "        # TODO: Initialze two target critic and one target actor networks and load the corresponding state_dicts\n",
        "        self.q1_target = Critic(obs_shape, act_shape).to(device)\n",
        "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
        "        self.q2_target = Critic(obs_shape, act_shape).to(device)\n",
        "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
        "        self.actor_target =  Actor(obs_shape, act_shape, env.action_space.low , env.action_space.high).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "\n",
        "        # TODO: Create ADAM optimizer for the Critic and Actor networks\n",
        "        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=lr)\n",
        "        self.q2_optimizer =  optim.Adam(self.q2.parameters(), lr=lr)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.ent_coef_optimizer = optim.Adam([self.log_ent_coef], lr=lr)\n",
        "\n",
        "        #self.log_ent_coef  =  self.log_ent_coef.to(device)\n",
        "\n",
        "\n",
        "    def train(self, num_episodes: int) -> EpisodeStats:\n",
        "        \"\"\"\n",
        "        Train the SAC agent.\n",
        "\n",
        "        :param num_episodes: Number of episodes to train.\n",
        "        :returns: The episode statistics.\n",
        "        \"\"\"\n",
        "        # Keeps track of useful statistics\n",
        "        stats = EpisodeStats(\n",
        "            episode_lengths=np.zeros(num_episodes),\n",
        "            episode_rewards=np.zeros(num_episodes),\n",
        "        )\n",
        "        current_timestep = 0\n",
        "\n",
        "        for i_episode in range(num_episodes):\n",
        "            # Print out which episode we're on, useful for debugging.\n",
        "            if (i_episode + 1) % 100 == 0:\n",
        "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}')\n",
        "\n",
        "            # Reset the environment and get initial observation\n",
        "            obs, _ = self.env.reset()\n",
        "\n",
        "            for episode_time in itertools.count():\n",
        "                # Choose action and execute\n",
        "                with torch.no_grad():\n",
        "                    action, _ = self.actor(torch.as_tensor(obs).float().unsqueeze(0).to(device))\n",
        "                    action = action.cpu().numpy().clip(self.env.action_space.low, self.env.action_space.high)\n",
        "                    action = action.squeeze(0)\n",
        "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "                # Update statistics\n",
        "                stats.episode_rewards[i_episode] += reward\n",
        "                stats.episode_lengths[i_episode] += 1\n",
        "\n",
        "                # Store sample in the replay buffer\n",
        "                self.buffer.store(\n",
        "                    torch.as_tensor(obs, dtype=torch.float32),\n",
        "                    torch.as_tensor(action),\n",
        "                    torch.as_tensor(reward, dtype=torch.float32),\n",
        "                    torch.as_tensor(next_obs, dtype=torch.float32),\n",
        "                    torch.as_tensor(terminated),\n",
        "                )\n",
        "\n",
        "                # Sample a mini batch from the replay buffer\n",
        "                obs_batch, act_batch, rew_batch, next_obs_batch, tm_batch = self.buffer.sample(self.batch_size)\n",
        "\n",
        "                obs_batch =  obs_batch.to(device)\n",
        "                rew_batch =  rew_batch.to(device)\n",
        "                tm_batch =  tm_batch.to(device)\n",
        "                act_batch =  act_batch.to(device)\n",
        "                next_obs_batch =  next_obs_batch.to(device)\n",
        "\n",
        "                # Update the Critic network\n",
        "                update_critics(\n",
        "                    self.q1,\n",
        "                    self.q1_target,\n",
        "                    self.q1_optimizer,\n",
        "                    self.q2,\n",
        "                    self.q2_target,\n",
        "                    self.q2_optimizer,\n",
        "                    self.actor_target,\n",
        "                    self.log_ent_coef,\n",
        "                    self.gamma,\n",
        "                    obs_batch,\n",
        "                    act_batch,\n",
        "                    rew_batch,\n",
        "                    next_obs_batch,\n",
        "                    tm_batch\n",
        "                )\n",
        "\n",
        "\n",
        "                # Update the Actor network\n",
        "                update_actor(\n",
        "                    self.q1,\n",
        "                    self.q2,\n",
        "                    self.actor,\n",
        "                    self.actor_optimizer,\n",
        "                    obs_batch.float(),\n",
        "                    self.log_ent_coef,\n",
        "                )\n",
        "\n",
        "                # Update Entropy Coefficient\n",
        "                update_entropy_coefficient(\n",
        "                    self.actor,\n",
        "                    self.log_ent_coef,\n",
        "                    self.target_entropy,\n",
        "                    self.ent_coef_optimizer,\n",
        "                    obs_batch.float(),\n",
        "                )\n",
        "\n",
        "                # Update the target networks via Polyak Update\n",
        "                polyak_update(self.q1.parameters(), self.q1_target.parameters(), self.tau)\n",
        "                polyak_update(self.q2.parameters(), self.q2_target.parameters(), self.tau)\n",
        "                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n",
        "\n",
        "                current_timestep += 1\n",
        "\n",
        "                # Check whether the episode is finished\n",
        "                if terminated or truncated or episode_time >= 500:\n",
        "                    break\n",
        "                obs = next_obs\n",
        "        return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMZIBwTlcIC6"
      },
      "source": [
        "## 1.5 Training\n",
        "We train now on the Car Racing environment from (Gymnasium)[https://gymnasium.farama.org/environments/box2d/car_racing/]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU9HY692cIC6",
        "outputId": "3676db82-6498-46b0-9d77-603d5fab1204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on CarRacing-v2\n",
            "Observation space: Box(0, 255, (96, 96, 3), uint8)\n",
            "Action space: Box([-1.  0.  0.], 1.0, (3,), float32)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Choose your environment\n",
        "env = gym.make(\"CarRacing-v2\",                 \n",
        "               domain_randomize=True,\n",
        "               continuous=True)\n",
        "\n",
        "# Print observation and action space infos\n",
        "print(f\"Training on {env.spec.id}\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "# Hyperparameters, Hint: Change as you see fit\n",
        "LR = 0.0001\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_BUFFER_SIZE = 100_000\n",
        "TAU = 0.005\n",
        "NUM_EPISODES = 3\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "TARGET_ENTROPY = -1.0\n",
        "\n",
        "# Train SAC\n",
        "agent = SACAgent(\n",
        "    env,\n",
        "    gamma=DISCOUNT_FACTOR,\n",
        "    lr=LR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    tau=TAU,\n",
        "    maxlen=REPLAY_BUFFER_SIZE,\n",
        "    target_entropy=TARGET_ENTROPY,\n",
        ")\n",
        "stats = agent.train(NUM_EPISODES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCwYa-NbcIC7"
      },
      "source": [
        "### Save and Load the trained actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jnymfyjcIC7",
        "outputId": "6492559a-3692-4517-b90f-002cf583ae38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actor(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=259200, out_features=256, bias=True)\n",
            "  (fc_mu): Linear(in_features=256, out_features=3, bias=True)\n",
            "  (fc_std): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# save the trained actor\n",
        "torch.save(agent.actor, \"sac_actor.pt\")\n",
        "\n",
        "# loading the trained actor\n",
        "loaded_actor = torch.load(\"sac_actor.pt\")\n",
        "loaded_actor.eval()\n",
        "print(loaded_actor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJfyoLhZcIC7"
      },
      "source": [
        "# 1.5 Results\n",
        "\n",
        "Like in the last exercise, we will look at the resulting episode reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "PalXP9CscIC7",
        "outputId": "a3c9fca7-d29b-4b5e-f62e-23439834ced5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "smoothing_window=20\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
        "\n",
        "# Plot the episode length over time\n",
        "ax = axes[0]\n",
        "ax.plot(stats.episode_lengths)\n",
        "ax.set_xlabel(\"Episode\")\n",
        "ax.set_ylabel(\"Episode Length\")\n",
        "ax.set_title(\"Episode Length over Time\")\n",
        "\n",
        "# Plot the episode reward over time\n",
        "ax = axes[1]\n",
        "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
        "ax.plot(rewards_smoothed)\n",
        "ax.set_xlabel(\"Episode\")\n",
        "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
        "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZTSaow5cIC7"
      },
      "source": [
        "Lastly, let us see what the learned policy does in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "-2psdv32cIC7",
        "outputId": "f2531a60-377e-40d9-ade8-1325c8c3b3c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fabian/anaconda3/envs/rl_lab/lib/python3.9/site-packages/gymnasium/envs/box2d/car_racing.py:582: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 1 is out of bounds for axis 0 with size 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_12179/1172600526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendered_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0msave_rgb_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trained.gif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mIImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"trained.gif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_12179/1172600526.py\u001b[0m in \u001b[0;36mrendered_rollout\u001b[0;34m(policy, env, max_steps)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/rl_lab/lib/python3.9/site-packages/gymnasium/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/rl_lab/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/rl_lab/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/rl_lab/lib/python3.9/site-packages/gymnasium/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image as IImage\n",
        "\n",
        "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
        "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
        "    # Create a list to hold each frame\n",
        "    frames = []\n",
        "\n",
        "    # Convert RGB arrays to PIL Image objects\n",
        "    for rgb_array in rgb_arrays:\n",
        "        rgb_array = (rgb_array).astype(np.uint8)\n",
        "        img = Image.fromarray(rgb_array)\n",
        "        frames.append(img)\n",
        "\n",
        "    # Save the frames as an animated GIF\n",
        "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
        "\n",
        "def rendered_rollout(policy, env, max_steps=1_000):\n",
        "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    imgs = [env.render()]\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            action = policy(torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0))[0] #.cpu().numpy()\n",
        "            action  = action.squeeze(0)\n",
        "            action   = action.to('cpu').numpy()\n",
        "\n",
        "        obs, _, terminated, truncated, _ = env.step(action)\n",
        "        obs = torch.tensor(obs).to(device)\n",
        "        obs  = obs.unsqueeze(0)\n",
        "        \n",
        "        imgs.append(env.render())\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    return imgs\n",
        "\n",
        "imgs = rendered_rollout(loaded_actor, env)\n",
        "save_rgb_animation(imgs, \"trained.gif\")\n",
        "IImage(filename=\"trained.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPLt58OFcIC8"
      },
      "source": [
        "We did not train very far because it can be very time-consuming.\n",
        "\n",
        "---\n",
        "<span style=\"color:orange\">**EXERCISE**</span>: **RL Project Preperation**\n",
        "\n",
        "1. What aspects of the problem should be first considered when trying to use RL for a particular task? (1 point)\n",
        "2. Which quantities can be monitored during training in order to observe progress and help with debugging? (1 point)\n",
        "3. What are some real world applications of RL? (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FH7RuegcIC8"
      },
      "source": [
        "**Your answers:**\n",
        "1. Some aspects to consider are whether the state and actions are discrete or continous. Basic q learning is useful if state and actions are discrete and small. If the state space is much larger, we can use DQN. For continous actions we can use ddpg. Another thing to consider is if we want a more generalised policy, in which case SAC is useful.\n",
        "2. We can monitor the episode count and also the time taken to training to help with debugging\n",
        "3. Some application incuded autonomous driving and robotic control both needs to deal with continous actions and states. Others include games like Go which have a discrete space and actions. It can be used in finance for market analysis. It can also be used in NLP to generate text."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
